{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nearest Centroid ML model Lab\n",
    "\n",
    "\n",
    "**Note:**\n",
    "1. Mandatory pre-requisite: Intimate understanding of Numpy tutorial\n",
    "2. Numpy Documentation - User Guide & API Reference: https://numpy.org/doc/stable/\n",
    "3. Cautionary Note: Watch out for the version of numpy you are using and the documentation you are referring\n",
    "\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "**We will apply sklearn Nearest Centroid to Iris dataset**\n",
    "\n",
    "1. [What is inside the 'iris' object?](#inside-iris)\n",
    "2. [Deciding on feature importance and select two features](#pca-select-two)\n",
    "3. [Preparing data for Nearest Centroid implementation with 2 features](#prepare-date)\n",
    "4. [Visualizing the 2 important features](#visualize-features)\n",
    "5. [NearestCentroid model fitting for two features with sklearn implementation](#sklearn-nearest-centroid-2features)\n",
    "6. [NearestCentroid model fitting for all features with sklearn implementation](#sklearn-nearest-centroid-4features)\n",
    "7. [NearestCentroid model sklearn with stratified split](#sklearn-nearest-centroid-split2)\n",
    "8. [Practice Problems](#masking)\n",
    "9. [Saving the model](#saving-model)\n",
    "10. [Classroom exercise/Mandatory Assignment 1 - Nearest Centroid from scratch](#aml-assignment1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='python-datatypes-size'></a>\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(np.__version__) #Check the numpy version for sanity check\n",
    "print(sk.__version__)\n",
    "print(pd.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"nearest-centroid\"></a>\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nearest Centroid\n",
    "\n",
    "Consider a binary classicication dataset - positive and negative samples. In 2D, this can be as follows:\n",
    "\n",
    "<img src=\"https://onedrive.live.com/embed?resid=A5A4158EF1352FCB%211865&authkey=%21AGUSpyxdsi3H9E4&width=634&height=300\" alt=\"Nearest Centroid\" style=\"width:700px;\"/>\n",
    "\n",
    "1. The mu+ and mu- represent the centroid of the positive and negative samples.\n",
    "2. Given any new point, we can plot it and see which centroid is closer to it and make the prediction accordingly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"inside-iris\"></a>\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 What is inside the 'iris' object?\n",
    "\n",
    "1. Let us use the iris dataset to apply Nearest Centroid algorithm\n",
    "2. Iris dataset is the \"Hello world\" of machine learning for data scientists\n",
    "3. Iris is a flowering plant genus with 310 species \n",
    "4. Three species of Iris genus are covered in Iris dataset - setosa, versicolor, and virginica\n",
    "5. Sepal and petal length and width can tell these species apart. Measurements for 150 samples was collected and documented\n",
    "\n",
    "<img src=\"https://onedrive.live.com/embed?resid=A5A4158EF1352FCB%211861&authkey=%21AG_HQ0dmbbf4Fxk&width=367&height=137\" alt=\"Iris dataset species\" style=\"width:700px;\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"type(iris)={type(iris)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(iris) #https://docs.python.org/3/library/functions.html#dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced users only. Skip this if you feel uncomfortable\n",
    "#vars(iris)\n",
    "\n",
    "# https://docs.python.org/3/reference/datamodel.html\n",
    "#iris.__dict__\n",
    "\n",
    "#https://stackoverflow.com/questions/191010/how-to-get-a-complete-list-of-objects-methods-and-attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "print(f\"type(X)={type(X)}\")\n",
    "print(f\"Shape of X ={X.shape}\")\n",
    "print(f\"type(y)={type(y)}\")\n",
    "print(f\"y.shape={y.shape}\")\n",
    "#\n",
    "print(\"Feature names are\")\n",
    "print(iris.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.DESCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"pca-select-two\"></a>\n",
    "<hr/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Deciding on feature importance and select two features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the iris dataset to a pandas dataframe\n",
    "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "\n",
    "# Add the target variable to the dataframe\n",
    "df['target'] = iris.target\n",
    "\n",
    "# Print the first 5 rows of the dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"target\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1 Deciding on feature importance\n",
    "\n",
    "1. For demonstration purposes we want to select two features that are most important in Iris species prediction\n",
    "2. This is so that we can show it on 2D plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "\n",
    "# Add the target variable to the dataframe\n",
    "df['target'] = iris.target\n",
    "\n",
    "# Print the first 5 rows of the dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "ss = StandardScaler()\n",
    "X_train_scaled = ss.fit_transform(X_train)\n",
    "X_test_scaled = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA().fit(X_train_scaled)\n",
    "\n",
    "plt.plot(pca.explained_variance_ratio_.cumsum(), lw=3, color='#087E8B')\n",
    "plt.title('Cumulative explained variance by number of principal components', size=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadings = pd.DataFrame(\n",
    "    data=pca.components_.T * np.sqrt(pca.explained_variance_), \n",
    "    columns=[f'PC{i}' for i in range(1, len(X_train.columns) + 1)],\n",
    "    index=X_train.columns\n",
    ")\n",
    "loadings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc1_loadings = loadings.sort_values(by='PC1', ascending=False)[['PC1']]\n",
    "pc1_loadings = pc1_loadings.reset_index()\n",
    "pc1_loadings.columns = ['Attribute', 'CorrelationWithPC1']\n",
    "\n",
    "plt.bar(x=pc1_loadings['Attribute'], height=pc1_loadings['CorrelationWithPC1'], color='#087E8B')\n",
    "plt.title('PCA loading scores (first principal component)', size=20)\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"prepare-date\"></a>\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Preparing data for Nearest Centroid implementation with 2 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "\n",
    "# Add the target variable to the dataframe\n",
    "df['target'] = iris.target\n",
    "\n",
    "# Print the first 5 rows of the dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_col = df[\"petal length (cm)\"] # How to fetch a single df column\n",
    "print(type(df_col))\n",
    "\n",
    "df[\"petal length (cm)\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to fetch dataframe for a specific set of columns\n",
    "cols = [\"petal length (cm)\", \"petal width (cm)\", \"target\"]\n",
    "df[cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#notice the use of double parenthesis for holding list of columns\n",
    "df_2d = df[[\"petal length (cm)\", \"petal width (cm)\", \"target\"]]\n",
    "df_2d.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head() #Original dataframe still contains all columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extracting X and y**\n",
    "\n",
    "1. Our goal while implmenting this ML model is still to get a strong grip of Numpy.\n",
    "2. Because of this, even though we created a Pandas dataframe earlier, we will approach its underlying numpy arrays instead of taking advantage of Pandas features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_col = 'target'\n",
    "y = df_2d[y_col]\n",
    "\n",
    "print(type(y))\n",
    "print(y.shape)\n",
    "y[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#another non idemtpotent operation. running this cell second time wont work\n",
    "y = y.to_numpy() \n",
    "\n",
    "print(type(y))\n",
    "print(y.shape)\n",
    "y[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_2d[[\"petal length (cm)\", \"petal width (cm)\"]].to_numpy()\n",
    "X[0:5,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: \n",
    "1. Above we explicitly specified the X and y columns. It is going to be extremely hard in future when we have hundreds of columns.\n",
    "2. Instead we will use a generic method that knowing only the target field and what features to remove  is enough\n",
    "3. CAUTION: Should be aware that the pop() method below is a non-idempotent operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2d.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a mutable step and non idempotent operation altering df_2d itself\n",
    "# Which means you cannot go a few cells back and start executing \n",
    "# You are bound to see different results based on where in the previous cells you start\n",
    "y = df_2d.pop(\"target\").to_numpy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2d.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_2d.to_numpy()\n",
    "X[0:5,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"visualize-features\"></a>\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Visualizing the 2 important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "df['target'] = iris.target\n",
    "df_2d = df[[\"petal length (cm)\", \"petal width (cm)\", \"target\"]]\n",
    "df_2d.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_2d.pop(\"target\").to_numpy()\n",
    "X = df_2d.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, shuffle = True, random_state = 0)\n",
    " \n",
    "print(f\"size of X_train = {X_train.shape[0]}\")\n",
    "# TODO: Add prints for others and check if the split is right "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us example what exactly does train_test_split return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = train_test_split(X, y, test_size = 0.2, shuffle = True, random_state = 0)\n",
    "\n",
    "# TODO: Add code here to examine the contents of output\n",
    "\n",
    "# 1. What is the first step in examining the contents?\n",
    "# Hint: check the type\n",
    "\n",
    "# 2. If iterable then check the elements in iterable\n",
    "\n",
    "\n",
    "# Conclusion: we did list unpacking on train_test_split !!\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall our earlier encounter with tuple unpacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tuple():\n",
    "    tup = (1,2,3,4)\n",
    "    return tup\n",
    "\n",
    "def get_tuple():\n",
    "    i = 1\n",
    "    j = 2\n",
    "    k = 3\n",
    "    l = 4\n",
    "    return (i,j,k,l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = get_tuple()\n",
    "output\n",
    "\n",
    "out1, out2, out3, out4 = get_tuple()\n",
    "\n",
    "# TODO: Write code here if we dont care about the first three outputs\n",
    "# Hint: Use underscores for the output you dont care\n",
    "out, _, _, _ = get_tuple()\n",
    "\n",
    "# for i, _ in enumerate(...):\n",
    "#     #do something\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why are we shuffling the dataset before splitting?**\n",
    "\n",
    "Ans: Data should be randomly distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df # See the output of this. Can you see the problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=\"red\">NOTE: We can shuffle only if the dataset is IID. Why?</font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualization: Plot the 2D graph**\n",
    "\n",
    "Note: Plotting centroid is missing. We will add it next after class based split of the numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = X_train[:,0]\n",
    "x2 = X_train[:,1]\n",
    "\n",
    "plt.scatter(x1, x2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(style='whitegrid')\n",
    "\n",
    "sns.scatterplot(x=x1, y=x2, hue=y_train)\n",
    "plt.legend()\n",
    "\n",
    "plt.title('Exploring Iris clusters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sklearn-nearest-centroid-2features\"></a>\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. NearestCentroid model fitting for two features with sklearn implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestCentroid\n",
    "\n",
    "# Create Nearest Centroid Classifier\n",
    "model_small = NearestCentroid()\n",
    "\n",
    "# Training the classifier\n",
    "model_small.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model_small.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Write your custom code to achieve the same result as \n",
    "# accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using model.score() method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing Accuracy on Training and Test sets\n",
    "print(f\"Training Set Score : {model_small.score(X_train, y_train) * 100} %\")\n",
    "print(f\"Test Set Score : {model_small.score(X_test, y_test) * 100} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Can we get those centroids from model coefficients?**\n",
    "\n",
    "Parametric models always give coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_small.centroids_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualization: Plot the 2D graph WITH centroids**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(style='whitegrid')\n",
    "\n",
    "sns.scatterplot(x=x1, y=x2, hue=y_train)\n",
    "sns.scatterplot(x=model_small.centroids_[:,0], y=model_small.centroids_[:,1], color=\"green\", marker=\"P\")\n",
    "plt.legend()\n",
    "\n",
    "plt.title('Exploring Iris clusters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Confusion matrix**\n",
    "\n",
    "False positives and true negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred, labels=model_small.classes_)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model_small.classes_)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_test.shape)\n",
    "#29/30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Show the incorrectly predicted data points in combined graph**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test != y_pred #This is an example of mask based indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrectly_predicted = X_test[y_test != y_pred]\n",
    "print(incorrectly_predicted)\n",
    "print(f\"Actual label = {y_test[y_test != y_pred]}, Predicted Label = {y_pred[y_test != y_pred]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(style='whitegrid')\n",
    "\n",
    "sns.scatterplot(x=x1, y=x2, hue=y_train)\n",
    "sns.scatterplot(x=model_small.centroids_[:,0], y=model_small.centroids_[:,1], color=\"green\")\n",
    "sns.scatterplot(x=incorrectly_predicted[:,0], y=incorrectly_predicted[:,1], color=\"red\", marker=\"x\")\n",
    "plt.legend()\n",
    "\n",
    "plt.title('Exploring Iris clusters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sklearn-nearest-centroid-4features\"></a>\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6. NearestCentroid model fitting for all features with sklearn implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "df['target'] = iris.target\n",
    "\n",
    "y = df.pop(\"target\").to_numpy()\n",
    "X = df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, shuffle = True, random_state = 0)\n",
    "\n",
    "# Create Nearest Centroid Classifier\n",
    "model_big = NearestCentroid()\n",
    "\n",
    "# Training the classifier\n",
    "model_big.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing Accuracy on Training and Test sets\n",
    "print(f\"Training Set Score : {model_big.score(X_train, y_train) * 100} %\")\n",
    "print(f\"Test Set Score : {model_big.score(X_test, y_test) * 100} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"blue\">NOTE: Earlier we used 2 features and consistenly got accuracy of 96.67%. Using 4 features is consistently giving us accuracy of 90%</font>\n",
    "\n",
    "**<font color=\"red\">Lesson learnt: The more, the merrier does not hold when it comes to ML features</font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sklearn-nearest-centroid-split2\"></a>\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7. NearestCentroid model sklearn with stratified split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "df['target'] = iris.target\n",
    "\n",
    "y = df.pop(\"target\").to_numpy()\n",
    "X = df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, stratify=y, shuffle = True, random_state = 0)\n",
    " \n",
    "print(f\"size of X_train = {X_train.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"fitpredict-sklearn\"></a>\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8. Practice Problems\n",
    "\n",
    "1. Write code to fit and predict using sklearn directly on df_train\n",
    "2. Write code to fit and predict nearest centroid on diabetes dataset\n",
    "\n",
    "NOTE: syntax of indexing, slicing and binary masking operations on pandas as same as numpy, except thatyou will use df.loc() and df.iloc() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"saving-model\"></a>\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 9. Saving the model\n",
    "\n",
    "1. Saving the model by pickling\n",
    "2. Saving the model with joblib\n",
    "3. Saving the model as json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_small.centroids_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "df['target'] = iris.target\n",
    "df_2d = df[[\"petal length (cm)\", \"petal width (cm)\", \"target\"]]\n",
    "df_2d.head()\n",
    "\n",
    "df_2d.pop(\"target\").to_numpy()\n",
    "X = df_2d.to_numpy()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, shuffle = True, random_state = 0)\n",
    "\n",
    "model_small = NearestCentroid()\n",
    "model_small.fit(X_train, y_train)\n",
    "y_pred = model_small.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "#save model using pickle\n",
    "model_filename = \"nearest_centroid_model_small.pkl\"\n",
    "with open(model_filename, 'wb') as file: \n",
    "    pickle.dump(model_small, file)  #give this model file to your friend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your friend does this to run your model\n",
    "with open(model_filename, \"rb\") as file:\n",
    "    model_object = pickle.load(file)\n",
    "\n",
    "dummy_iris_record = np.array([[5,4],[3,2]])\n",
    "model_object.predict(dummy_iris_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(model_object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Saving the model with joblib\n",
    "\n",
    "1. Joblib belongs SciPy ecosystem with utilities for pipelining Python jobs. \n",
    "2. Can be used to save/load Python objects that use lot of numpy data structures, very efficiently. \n",
    "3. This is useful in ML algorithms such as kNN that require to store a lot of parameters (in kNN this means to store entire dataset as model file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "model_filename = \"nearest_centroid_model_small.joblib\"\n",
    "joblib.dump(model_small, model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_object = joblib.load(model_filename)\n",
    "\n",
    "dummy_iris_record = np.array([[5,4],[3,2]])\n",
    "model_object.predict(dummy_iris_record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Save your model using Json\n",
    "\n",
    "To save as json, you should know what to save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "model_params = {}\n",
    "model_params[\"centroids\"] = model_small.centroids_\n",
    "\n",
    "json_text = json.dumps(model_params, indent=4) # this line fails. why?\n",
    "\n",
    "model_filename = \"nearest_centroid_model_small.json\"\n",
    "with open(model_filename, \"wb\") as file:\n",
    "    file.write(json_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "model_params = {}\n",
    "model_params[\"centroids\"] = model_small.centroids_.tolist() #numpy array cannot be written by json library\n",
    "\n",
    "json_text = json.dumps(model_params, indent=4)\n",
    "#json_text\n",
    "\n",
    "model_filename = \"nearest_centroid_model_small.json\"\n",
    "with open(model_filename, \"wb\") as file: \n",
    "    file.write(json_text) # this line fails. why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.getcwd() #Current working directory. this is where pkl file will be saved "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "model_params = {}\n",
    "model_params[\"centroids\"] = model_small.centroids_.tolist() \n",
    "\n",
    "json_text = json.dumps(model_params, indent=4)\n",
    "\n",
    "model_filename = \"nearest_centroid_model_small.json\"\n",
    "with open(model_filename, \"w\") as file:\n",
    "    file.write(json_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read the json file and create model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(model_filename, 'r') as f: #notice the file is read as text by using 'r' option instead of 'rb'\n",
    "  data = json.load(f)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids = data[\"centroids\"]\n",
    "centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NearestCentroid()\n",
    "model.centroids_ = centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_iris_record = np.array([[5,4],[3,2]])\n",
    "model.predict(dummy_iris_record) #this fails. why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Open the json text file and see. What problem do you see?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "model_params = {}\n",
    "model_params[\"centroids\"] = model_small.centroids_.tolist()\n",
    "model_params[\"labels\"] = model_small.classes_.tolist()\n",
    "\n",
    "\n",
    "json_text = json.dumps(model_params, indent=4)\n",
    "model_filename = \"nearest_centroid_model_small.json\"\n",
    "with open(model_filename, \"w\") as file:\n",
    "    file.write(json_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(model_filename, 'r') as f: #notice the file is read as text by using 'r' option instead of 'rb'\n",
    "  data = json.load(f)\n",
    "\n",
    "model = NearestCentroid()\n",
    "model.centroids_ = data[\"centroids\"]\n",
    "model.classes_ = data[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_iris_record = np.array([[5,4],[3,2]])\n",
    "model.predict(dummy_iris_record) #this fails. why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(model.centroids_))\n",
    "print(type(model.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(model_filename, 'r') as f: # notice the file is read as text by using 'r' option instead of 'rb'\n",
    "  data = json.load(f)\n",
    "\n",
    "model = NearestCentroid()\n",
    "model.centroids_ = np.array(data[\"centroids\"]) # notice that we are converting to np array from list\n",
    "model.classes_ = np.array(data[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_iris_record = np.array([[5,4],[3,2]])\n",
    "model.predict(dummy_iris_record) #this works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Limitations of NearestCentroid model**\n",
    "\n",
    "Assymetrical distribution gives errorneous predictions with Nearest Centroid model\n",
    "\n",
    "![Assymetrical distribution gives errorneous predictions](https://onedrive.live.com/embed?resid=A5A4158EF1352FCB%211866&authkey=%21AKPIbnjIITqB48w&width=522&height=350)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"aml-assignment1\"></a>\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 10. AML class exercise/mandatory assignment 1\n",
    "\n",
    "In the course of this lab, you used NearestCentroid class from sklearn library and used it to make predictions from scratch\n",
    "Now you have the knowledge to write a similar NearestCentroid from scratch.\n",
    "\n",
    "Your custom NearestCentroid implementation should be able to accept any dataset as input, with any number of labels and do the prediction.\n",
    "But first, code your custom Nearest Centroid implementation specifically for Iris dataset with 3 labels and then generalize for n labels.\n",
    "\n",
    "1. Separate the dataset into n labels by using the boolean mask based indexing\n",
    "2. Calculate the centroid of each class. \n",
    "3. For any incoming test data, check the distance of each test data point from the centroid. Each test data point belongs to that class to whose centroid it is closest\n",
    "4. For the given train test split, verify your code prediction is same as sklearn NearestCentroid prediction \n",
    "5. **<font color=\"red\">Write the code as reusable Python classes along the lines of sklearn classes (but dont aim for it at the outset)</font>**\n",
    "\n",
    "Hint: \n",
    "1. To calculate the distance between any two data points a and b, use the np.linalg.norm(a-b). In this case distance between all test points and all centroids should be calculated.\n",
    "2. You can implement this with the traditional two nested for loops. Or if you can use vectorization  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import NearestCentroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "df['target'] = iris.target\n",
    "\n",
    "y = df.pop(\"target\").to_numpy()\n",
    "X = df.to_numpy()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, shuffle = True, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sk = NearestCentroid()\n",
    "model_sk.fit(X_train, y_train)\n",
    "y_pred = model_sk.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sk.centroids_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNearestCentroid:\n",
    "    def fit(self, X, y):\n",
    "        self.labels = np.unique(y)\n",
    "        self.centroids = []\n",
    "        for lbl in self.labels:\n",
    "            y_lbl = y[y==lbl]\n",
    "            X_lbl = X[y==lbl]\n",
    "            centroid = np.mean(X_lbl, axis=0)\n",
    "            self.centroids.append(centroid)\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "        prediction with traditional nested loop\n",
    "        This function has a sneaky bug that prevents it from working as expected.\n",
    "        Identify & fix the bug & ping me your answers. This is first part of AML assignment 1\n",
    "    \"\"\"\n",
    "    def predict(self, X):\n",
    "        num_records = X.shape[0]\n",
    "        y_pred_distances = np.empty((num_records, len(self.labels)))\n",
    "        for i in np.arange(0,num_records):\n",
    "            for j, centroid in enumerate(self.centroids):\n",
    "                y_pred_distances[i, j] = np.linalg.norm(X[i] - centroid[j], axis=0)\n",
    "\n",
    "        y_pred =  np.argmin(y_pred_distances, axis=1)\n",
    "        return y_pred\n",
    "    \n",
    "    \"\"\"\n",
    "        TODO: Add vectorized code to do prediction\n",
    "        This is second part of AML assignment 1\n",
    "    \"\"\"\n",
    "    def predict_vectorized(self, X):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mymodel = MyNearestCentroid()\n",
    "mymodel.fit(X_train, y_train)\n",
    "\n",
    "print(mymodel.labels)\n",
    "mymodel.centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = mymodel.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quickstart",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
